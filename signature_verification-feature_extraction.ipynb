{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all versions of a signature from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorottyahuszti/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "from sklearn import ensemble\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# CONSTANTS\n",
    "# path to signature's directory\n",
    "path = './SVC/Standard/006'\n",
    "\n",
    "# stores all versions of a signature\n",
    "signatures = []   \n",
    "# stores if it's original or fake (1 - original, 0 - fake)\n",
    "target = []\n",
    "\n",
    "# get circles for an image\n",
    "def getCircles(file):\n",
    "    image = cv2.imread(path + '/' + file,0)\n",
    "    image = cv2.medianBlur(image,5)\n",
    "    grayImage = cv2.cvtColor(image,cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # get circles center\n",
    "    circles = cv2.HoughCircles(image,cv2.HOUGH_GRADIENT,1,20,param1=200,param2=20,minRadius=0,maxRadius=30)\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    \n",
    "\n",
    "    for i in circles[0,:]:\n",
    "        cv2.circle(grayImage,(i[0],i[1]),i[2],(0,255,0),2)\n",
    "        # draw the center of the circle\n",
    "        cv2.circle(grayImage,(i[0],i[1]),2,(0,0,255),3)\n",
    "        # write the modified picture to file with circles on it\n",
    "        filename = file.split('.')\n",
    "        cv2.imwrite(filename[0] + '_'+'.png',grayImage)\n",
    "    \n",
    "\n",
    "# get circles for all the images and create new image from it with the drawn circles on it\n",
    "def getCirclesForAll(path):\n",
    "    circlesAll = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.find('.png') != -1:\n",
    "            getCircles(file)\n",
    "\n",
    "# read all the images from a directory\n",
    "for file in os.listdir(path):\n",
    "    if file.find('.png') != -1:\n",
    "        # gray scaling the image\n",
    "        #signature = color.rgb2gray(io.imread(path + '/' + file))\n",
    "        \n",
    "        image = cv2.imread(path + '/' + file,0)\n",
    "        image = cv2.medianBlur(image,5)\n",
    "        grayImage = cv2.cvtColor(image,cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        # reshape image matrice to list\n",
    "        #signatures.append(signature.reshape(1,-1))\n",
    "        signatures.append(image)\n",
    "        \n",
    "        \n",
    "        # create label to image whether it's the original or fake one (1 - original, 0 - fake)\n",
    "        if file.find('_e_') != -1:\n",
    "            target.append(1)\n",
    "        else:\n",
    "            target.append(0)\n",
    "\n",
    "            \n",
    "# creates images with circles on them\n",
    "getCirclesForAll(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating input data (x) and expected output (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Define variables\\nn_samples = len(signatures)\\n\\ny = np.asarray(target)\\nx = []\\n\\nfor img in range(0,len(signatures)):\\n    x.append(signatures[img][0])\\n    \\n# get longest image represantation and padding the rest of it\\nlongest_image = max(x, key=lambda k: len(k))\\nlongest_size = len(longest_image)\\n\\n\\n# white pixel\\nwhite = x[0][0]\\n# pad the difference\\nfor sig in range(0,len(x)):\\n    if len(x[sig]) < longest_size:\\n        # how many white pixels have to be added\\n        difference = longest_size - len(x[sig])\\n        \\n        # adding extra white pixels as padding\\n        for pad in range(0,difference):\\n            x[sig] = np.append(x[sig], white)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Define variables\n",
    "n_samples = len(signatures)\n",
    "\n",
    "y = np.asarray(target)\n",
    "x = []\n",
    "\n",
    "for img in range(0,len(signatures)):\n",
    "    x.append(signatures[img][0])\n",
    "    \n",
    "# get longest image represantation and padding the rest of it\n",
    "longest_image = max(x, key=lambda k: len(k))\n",
    "longest_size = len(longest_image)\n",
    "\n",
    "\n",
    "# white pixel\n",
    "white = x[0][0]\n",
    "# pad the difference\n",
    "for sig in range(0,len(x)):\n",
    "    if len(x[sig]) < longest_size:\n",
    "        # how many white pixels have to be added\n",
    "        difference = longest_size - len(x[sig])\n",
    "        \n",
    "        # adding extra white pixels as padding\n",
    "        for pad in range(0,difference):\n",
    "            x[sig] = np.append(x[sig], white)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.875\n",
      "[1 1 1 0 1 1 0 1]\n",
      "[1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "y = np.asarray(target)\n",
    "\n",
    "# base class for feature extraction with pipelining\n",
    "class Circles(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self,signatures,y=y):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self,signatures,y=y):\n",
    "        return self.transform(signatures,y)\n",
    "\n",
    "    def transform(self,signatures,y=y):\n",
    "        circlesAll = []\n",
    "        \n",
    "        for i in range(len(signatures)):\n",
    "            circles = cv2.HoughCircles(signatures[i],cv2.HOUGH_GRADIENT,1,20,param1=200,param2=20,minRadius=0,maxRadius=30)\n",
    "            circles = np.uint16(np.around(circles))\n",
    "            \n",
    "            circlesArray = []\n",
    "            for j in circles[0,:]:\n",
    "                circlesArray.append(j[0])\n",
    "                circlesArray.append(j[1])\n",
    "            circlesAll.append(circlesArray)\n",
    "            \n",
    "           \n",
    "        longest_element = max(circlesAll, key=lambda k: len(k))\n",
    "        #longest_size = len(longest_element)\n",
    "        # ha feature szám nem egyezik hibát kapok, akkor ezt kell átállítani pl egy nagy számra, amit tuti nem ér el\n",
    "        longest_size = 12\n",
    "        \n",
    "        for i in range(len(circlesAll)):\n",
    "            if len(circlesAll[i]) < longest_size:\n",
    "                # how many white pixels have to be added\n",
    "                difference = longest_size - len(circlesAll[i])\n",
    "        \n",
    "                # adding extra white pixels as padding\n",
    "                for pad in range(0,difference):\n",
    "                    circlesAll[i].append(0)\n",
    "                    \n",
    "        return circlesAll\n",
    "    \n",
    "\n",
    "# define pipeline\n",
    "pipeline = Pipeline([\n",
    "    # define feature extraction\n",
    "    ('features',FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('circles', Circles())\n",
    "        ])\n",
    "    ),\n",
    "    # define model\n",
    "    ('classifier',RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# datasets\n",
    "x_train = signatures[:32]\n",
    "y_train = y[:32]\n",
    "x_test = signatures[32:]\n",
    "y_test = y[32:]\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "#cross_val_score(estimator=pipeline, X=x, y=y, scoring='accuracy')\n",
    "#scores = cross_validation.cross_val_score(pipeline, signatures[:32], y[:32], cv=3)\n",
    "#print(scores.mean())\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'classifier__n_estimators': n_estimators,\n",
    "               'classifier__max_depth': max_depth,\n",
    "               'classifier__min_samples_split': min_samples_split,\n",
    "               'classifier__min_samples_leaf': min_samples_leaf,\n",
    "               'classifier__bootstrap': bootstrap}\n",
    "\n",
    "#rf_random = RandomizedSearchCV(estimator = pipeline, param_distributions = random_grid, n_iter = 1, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "pipeline.fit(X=x_train,y=y_train)\n",
    "predictions = pipeline.predict(x_test)\n",
    "accuracy = pipeline.score(x_test,y_test)\n",
    "\n",
    "print('accuracy:',accuracy)\n",
    "print(predictions)\n",
    "print(y_test)\n",
    "\n",
    "###############################\n",
    "# using the best paramcombination from RandomizedSearchCV\n",
    "#model = rf_random.fit(signatures[:32], y[:32])\n",
    "#rf_random.best_params_\n",
    "\n",
    "# gets the pipeline tuning parameters\n",
    "#pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling train and validation data\n",
    "TODO random sampling (not in order,shuffle before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Using the Random Forest Classifier\n",
    "classifier = ensemble.RandomForestClassifier()\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "#                     hidden_layer_sizes=(5, 2), random_state=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signature images weren't the same size, so I have to resize them to standardize. It causes problem for the model when one image is 126000 pixel and the other is 127800. Especially when I converted the images to vectors and they didn't have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tree Classifier:\n",
      "\n",
      "Score\t0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorottyahuszti/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Fit model\n",
    "# the problem was with fit method, that the svc database contains the images in different size (one is 126000, the other is 127800)\n",
    "# the solution is to standardize the pictures to the same size\n",
    "classifier.fit(x_train,y_train)\n",
    "#clf.fit(x_train, y_train)\n",
    "\n",
    "#Attempt to predict validation data\n",
    "score = classifier.score(x_test, y_test)\n",
    "#score = clf.score(x_test,y_test)\n",
    "print('Random Tree Classifier:\\n') \n",
    "print('Score\\t'+str(score))\n",
    "\n",
    "i=0\n",
    "\n",
    "# prediction if it's the original or the fake one (1 is original, 0 is fake)\n",
    "classifier.predict(x_test[i])\n",
    "#clf.predict(x_test[i])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
